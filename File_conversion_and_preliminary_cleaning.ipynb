{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanjal02/Early-Disease-Prediction/blob/main/File_conversion_and_preliminary_cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xport"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8pE1B20VG8t",
        "outputId": "5d8cc1ce-7f0a-4ebe-f113-25f76645b07b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xport\n",
            "  Downloading xport-3.6.1-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: click>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from xport) (8.1.8)\n",
            "Collecting pandas<1.4,>=1.3.5 (from xport)\n",
            "  Downloading pandas-1.3.5.tar.gz (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from xport) (6.0.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from pandas<1.4,>=1.3.5->xport) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.11/dist-packages (from pandas<1.4,>=1.3.5->xport) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.11/dist-packages (from pandas<1.4,>=1.3.5->xport) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7.3->pandas<1.4,>=1.3.5->xport) (1.17.0)\n",
            "Downloading xport-3.6.1-py2.py3-none-any.whl (29 kB)\n",
            "Building wheels for collected packages: pandas\n",
            "  Building wheel for pandas (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pandas: filename=pandas-1.3.5-cp311-cp311-linux_x86_64.whl size=37464137 sha256=7923237f2959c8af0c337a7250fd5fa17b134bb836b948d22a82a23f1952b8de\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/e7/6d/d4c288f419ab8fa07c1db6f606a2ae18ecf3dc2839d79a1c07\n",
            "Successfully built pandas\n",
            "Installing collected packages: pandas, xport\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.3.5 which is incompatible.\n",
            "bigframes 1.35.0 requires pandas>=1.5.3, but you have pandas 1.3.5 which is incompatible.\n",
            "mizani 0.13.1 requires pandas>=2.2.0, but you have pandas 1.3.5 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.3.5 which is incompatible.\n",
            "xarray 2025.1.2 requires pandas>=2.1, but you have pandas 1.3.5 which is incompatible.\n",
            "statsmodels 0.14.4 requires pandas!=2.1.0,>=1.4, but you have pandas 1.3.5 which is incompatible.\n",
            "arviz 0.20.0 requires pandas>=1.5.0, but you have pandas 1.3.5 which is incompatible.\n",
            "cudf-cu12 24.12.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.3.5 which is incompatible.\n",
            "ibis-framework 9.2.0 requires pandas<3,>=1.5.3, but you have pandas 1.3.5 which is incompatible.\n",
            "geopandas 1.0.1 requires pandas>=1.4.0, but you have pandas 1.3.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-1.3.5 xport-3.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Gko-jWQSWcq"
      },
      "outputs": [],
      "source": [
        "# Required Imports\n",
        "import xport, csv\n",
        "import os\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JejOhjbJSWcs"
      },
      "outputs": [],
      "source": [
        "# Directory path\n",
        "data_dir = '/content/VID_L.xpt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjgPa0TFSWcs"
      },
      "outputs": [],
      "source": [
        "import xport, csv\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Function to convert an .XPT file to .csv format\n",
        "def xpt_to_csv(filepath, savedir):\n",
        "    with open(filepath, 'rb') as f:\n",
        "        # Overriding the default encoding\n",
        "        xport.v56.TEXT_METADATA_ENCODING = 'latin-1'  # or 'utf-8' if latin-1 doesn't work\n",
        "        df = xport.to_dataframe(f)\n",
        "    print(df.columns.values)\n",
        "    name = os.path.basename(filepath).split('.')[0] # Get the filename without the extension\n",
        "    savepath = os.path.join(savedir, name + '.csv') # Construct the save path using os.path.join\n",
        "    df.to_csv(savepath, index=False) # Save the DataFrame to CSV, avoid saving index\n",
        "\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xpt_to_csv(data_dir, '.') # Call the xpt_to_csv function with the file path and save directory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDrufjwKZ-fU",
        "outputId": "a790d9f3-db5a-4ef3-b441-d55e77ba4d35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['SEQN' 'WTPH2YR' 'LBXVIDMS' 'LBDVIDLC' 'LBXVD2MS' 'LBDVD2LC' 'LBXVD3MS'\n",
            " 'LBDVD3LC' 'LBXVE3MS' 'LBDVE3LC']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-b4dffH-Uhvq",
        "outputId": "686ec795-1a00-4ab3-9a98-ebd574c614b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def merge_files_on_seqn(file_list, output_file):\n",
        "    \"\"\"Merges multiple CSV files on the SEQN column, keeping all columns.\"\"\"\n",
        "    merged_df = None  # Initialize an empty DataFrame\n",
        "\n",
        "    for filename in file_list:\n",
        "        try:\n",
        "            df = pd.read_csv(filename)\n",
        "\n",
        "            # Ensure SEQN column exists\n",
        "            if 'SEQN' not in df.columns:\n",
        "                print(f\"Warning: 'SEQN' column not found in {filename}. Skipping file.\")\n",
        "                continue\n",
        "\n",
        "            # Merge on SEQN, using outer join to keep all columns\n",
        "            if merged_df is None:\n",
        "                merged_df = df  # First file sets the base dataframe\n",
        "            else:\n",
        "                merged_df = pd.merge(merged_df, df, on='SEQN', how='outer')\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: File not found: {filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "    # Save the merged dataframe\n",
        "    if merged_df is not None:\n",
        "        merged_df.to_csv(output_file, index=False)\n",
        "        print(f\"Files merged successfully into {output_file}\")\n",
        "    else:\n",
        "        print(\"No valid files found for merging.\")\n",
        "\n",
        "# Example usage:\n",
        "files_to_merge = [\n",
        "    '/content/Alcohol_Use.csv', '/content/Alpha-1_Glycoprotein.csv', '/content/Blood Pressure.csv',\n",
        "    '/content/Body Measures.csv', '/content/Complete_Blood_Count.csv', '/content/Current_Health_Stats.csv',\n",
        "    '/content/Demographics.csv', '/content/Diabetes.csv', '/content/Diet_Behavior&Nutrition.csv',\n",
        "    '/content/Fast_Questionnaire.csv', '/content/Feritin.csv', '/content/Folate(RBC).csv',\n",
        "    '/content/Glycohemoglobin.csv', '/content/Hep_A.csv', '/content/Hep_B_Surface_Antibody.csv',\n",
        "    '/content/High_Sensitivity_C_Protein.csv', '/content/Immunization.csv', '/content/Insulin.csv',\n",
        "    '/content/Lead, Cadmium, Total Mercury, Selenium, & Manganese.csv', '/content/Liver Ultrasound.csv',\n",
        "    '/content/Medical_Conditions.csv', '/content/Mercury_ Inorganic, Ethyl, and Methyl.csv',\n",
        "    '/content/Nutrient_Intake_day_1.csv', '/content/Nutrient_Intake_day_2.csv', '/content/Occupations.csv',\n",
        "    '/content/Physical_Activity.csv', '/content/Plasma_Fasting_Glucose.csv', '/content/Prescription_Meds.csv',\n",
        "    '/content/Serum_Folate.csv', '/content/Sex_Steroid_Hormone.csv', '/content/Sleep_Disorder.csv',\n",
        "    '/content/Smoking.csv', '/content/Total_Cholestrol.csv', '/content/Vitamin_D.csv', '/content/Weight_History.csv'\n",
        "]\n",
        "output_filename = 'merged_file.csv'\n",
        "merge_files_on_seqn(files_to_merge, output_filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZpWv5Fxad-B",
        "outputId": "d83ccaa0-8e72-414c-bc13-4f48e457622c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-92-122d451c490c>:21: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Unnamed: 0_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
            "  merged_df = pd.merge(merged_df, df, on='SEQN', how='outer')\n",
            "<ipython-input-92-122d451c490c>:21: FutureWarning: Passing 'suffixes' which cause duplicate columns {'WTPH2YR_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
            "  merged_df = pd.merge(merged_df, df, on='SEQN', how='outer')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files merged successfully into merged_file.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "# Download the merged_file.csv\n",
        "files.download('merged_file_clean.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "pE1yPEtMZ7eS",
        "outputId": "d835b732-5387-42c0-b415-f8016b669e98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_849df0f0-f911-44fd-ab85-da99be965cc0\", \"merged_file_clean.csv\", 19541154)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def identify_missing_columns(filepath, threshold=0.7):\n",
        "    \"\"\"\n",
        "    Identifies columns in a CSV file with more than the specified threshold of missing data.\n",
        "\n",
        "    Args:\n",
        "        filepath: Path to the CSV file.\n",
        "        threshold: The proportion of missing data (0 to 1) above which a column is flagged.\n",
        "\n",
        "    Returns:\n",
        "        A list of column names with more than the threshold proportion of missing values.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(filepath)\n",
        "        missing_percentage = df.isnull().sum() / len(df)\n",
        "        columns_to_drop = missing_percentage[missing_percentage > threshold].index.tolist()\n",
        "        return columns_to_drop\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {filepath}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return []\n",
        "\n",
        "# Example usage\n",
        "filepath = '/content/merged_file.csv'  # Replace with the actual file path\n",
        "missing_cols = identify_missing_columns(filepath)\n",
        "\n",
        "if missing_cols:\n",
        "    print(\"Columns with more than 70% missing data:\")\n",
        "    for col in missing_cols:\n",
        "        print(col)\n",
        "else:\n",
        "    print(\"No columns with more than 70% missing data found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-4Mks4Zb5zV",
        "outputId": "b7363ca1-b4a0-418c-edc5-48d314d0abf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns with more than 70% missing data:\n",
            "ALQ270\n",
            "ALQ280\n",
            "ALQ170\n",
            "WTPH2YR_x\n",
            "LBXAGP\n",
            "BMIWT\n",
            "BMXRECUM\n",
            "BMIRECUM\n",
            "BMXHEAD\n",
            "BMIHEAD\n",
            "BMIHT\n",
            "BMDBMIC\n",
            "BMILEG\n",
            "BMIARML\n",
            "BMIARMC\n",
            "BMIWAIST\n",
            "BMIHIP\n",
            "RIDAGEMN\n",
            "RIDEXAGM\n",
            "DMDYRUSR\n",
            "RIDEXPRG\n",
            "DMDHSEDZ\n",
            "DID040\n",
            "DIQ050\n",
            "DID060\n",
            "DIQ060U\n",
            "DIQ070\n",
            "DBQ010\n",
            "DBD030\n",
            "DBD041\n",
            "DBD050\n",
            "DBD055\n",
            "DBD061\n",
            "DBQ073A\n",
            "DBQ073B\n",
            "DBQ073C\n",
            "DBQ073D\n",
            "DBQ073E\n",
            "DBQ073U\n",
            "DBQ301\n",
            "DBQ330\n",
            "DBQ360\n",
            "DBQ370\n",
            "DBD381\n",
            "DBQ390\n",
            "DBQ400\n",
            "DBD411\n",
            "DBQ421\n",
            "DBQ424\n",
            "PHACOFHR\n",
            "PHACOFMN\n",
            "PHAALCHR\n",
            "PHAALCMN\n",
            "PHAGUMHR\n",
            "PHAGUMMN\n",
            "PHAANTHR\n",
            "PHAANTMN\n",
            "PHASUPHR\n",
            "PHASUPMN\n",
            "WTPH2YR_x.1\n",
            "LBXFER\n",
            "LBDFERSI\n",
            "IMQ060\n",
            "IMQ070\n",
            "IMQ090\n",
            "IMQ100\n",
            "LBXIN\n",
            "LBDINSI\n",
            "LBDINLC\n",
            "LUARXNC\n",
            "LUARXND\n",
            "LUARXIN\n",
            "MCQ035\n",
            "MCQ040\n",
            "MCQ050\n",
            "MCQ149\n",
            "MCQ195\n",
            "MCQ170M\n",
            "MCQ170L\n",
            "MCQ500\n",
            "MCQ510A\n",
            "MCQ510B\n",
            "MCQ510C\n",
            "MCQ510D\n",
            "MCQ510E\n",
            "MCQ510F\n",
            "MCQ230A\n",
            "MCQ230B\n",
            "MCQ230C\n",
            "MCQ230D\n",
            "DBD100\n",
            "DR1SKY\n",
            "DRQSDT1\n",
            "DRQSDT2\n",
            "DRQSDT3\n",
            "DRQSDT4\n",
            "DRQSDT5\n",
            "DRQSDT6\n",
            "DRQSDT7\n",
            "DRQSDT8\n",
            "DRQSDT9\n",
            "DRQSDT10\n",
            "DRQSDT11\n",
            "DRQSDT12\n",
            "DRQSDT91\n",
            "DRD350A\n",
            "DRD350AQ\n",
            "DRD350B\n",
            "DRD350BQ\n",
            "DRD350C\n",
            "DRD350CQ\n",
            "DRD350D\n",
            "DRD350DQ\n",
            "DRD350E\n",
            "DRD350EQ\n",
            "DRD350F\n",
            "DRD350FQ\n",
            "DRD350G\n",
            "DRD350GQ\n",
            "DRD350H\n",
            "DRD350HQ\n",
            "DRD350I\n",
            "DRD350IQ\n",
            "DRD350J\n",
            "DRD350JQ\n",
            "DRD350K\n",
            "DRD370A\n",
            "DRD370AQ\n",
            "DRD370B\n",
            "DRD370BQ\n",
            "DRD370C\n",
            "DRD370CQ\n",
            "DRD370D\n",
            "DRD370DQ\n",
            "DRD370E\n",
            "DRD370EQ\n",
            "DRD370F\n",
            "DRD370FQ\n",
            "DRD370G\n",
            "DRD370GQ\n",
            "DRD370H\n",
            "DRD370HQ\n",
            "DRD370I\n",
            "DRD370IQ\n",
            "DRD370J\n",
            "DRD370JQ\n",
            "DRD370K\n",
            "DRD370KQ\n",
            "DRD370L\n",
            "DRD370LQ\n",
            "DRD370M\n",
            "DRD370MQ\n",
            "DRD370N\n",
            "DRD370NQ\n",
            "DRD370O\n",
            "DRD370OQ\n",
            "DRD370P\n",
            "DRD370PQ\n",
            "DRD370Q\n",
            "DRD370QQ\n",
            "DRD370R\n",
            "DRD370RQ\n",
            "DRD370S\n",
            "DRD370SQ\n",
            "DRD370T\n",
            "DRD370TQ\n",
            "DRD370U\n",
            "DRD370UQ\n",
            "DRD370V\n",
            "DR2SKY\n",
            "OCQ210\n",
            "SMQ040\n",
            "SMD641\n",
            "SMD650\n",
            "SMD100MN\n",
            "SMQ621\n",
            "SMD630\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-94-4df163e38fbb>:30: DtypeWarning: Columns (13,197,509,512) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  missing_cols = identify_missing_columns(filepath)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: delete all the above identified columns\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def delete_columns(filepath, columns_to_delete):\n",
        "    \"\"\"Deletes specified columns from a CSV file and saves the result.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(filepath)\n",
        "\n",
        "        # Check if all columns to delete exist in the DataFrame\n",
        "        for col in columns_to_delete:\n",
        "            if col not in df.columns:\n",
        "                print(f\"Warning: Column '{col}' not found in the DataFrame. Skipping.\")\n",
        "\n",
        "        df = df.drop(columns=[col for col in columns_to_delete if col in df.columns])\n",
        "        df.to_csv(filepath.replace('.csv', '_clean.csv'), index=False)  # Save with _clean suffix\n",
        "        print(f\"Columns deleted and saved to {filepath.replace('.csv', '_clean.csv')}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {filepath}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Example usage (replace with the actual file path and columns to delete):\n",
        "filepath = '/content/merged_file.csv'\n",
        "# Assuming 'missing_cols' is the list of columns identified in the previous step\n",
        "delete_columns(filepath, missing_cols)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCpH2C3RcfYh",
        "outputId": "17dc38e5-7665-4617-a7bd-32047e1f750c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-95-99b9cd2d0c0d>:26: DtypeWarning: Columns (13,197,509,512) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  delete_columns(filepath, missing_cols)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns deleted and saved to /content/merged_file_clean.csv\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}